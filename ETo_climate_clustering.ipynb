{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulomarc49/ETo_climate/blob/main/ETo_climate_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WUC0UN0WQBt3"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import joblib\n",
        "import numpy  as np\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "import os\n",
        "from   tqdm                   import tqdm\n",
        "from   sklearn.preprocessing  import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mScBvV74JA50",
        "outputId": "17b39262-2514-453e-ff46-988e2386b7a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array for 2017 saved to /content/sample_data/ETo_climate_2017_dates.npy\n",
            "Array for 2018 saved to /content/sample_data/ETo_climate_2018_dates.npy\n",
            "Array for 2019 saved to /content/sample_data/ETo_climate_2019_dates.npy\n",
            "Array for 2020 saved to /content/sample_data/ETo_climate_2020_dates.npy\n",
            "Array for 2021 saved to /content/sample_data/ETo_climate_2021_dates.npy\n",
            "Array for 2022 saved to /content/sample_data/ETo_climate_2022_dates.npy\n"
          ]
        }
      ],
      "source": [
        "# Create dates array for ETo climate clusterization\n",
        "year_2017 = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]  # Number of days of each month of the year 2017\n",
        "year_2018 = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]  # Number of days of each month of the year 2018\n",
        "year_2019 = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]  # Number of days of each month of the year 2019\n",
        "year_2020 = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]  # Number of days of each month of the year 2020\n",
        "year_2021 = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]  # Number of days of each month of the year 2021\n",
        "year_2022 = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]  # Number of days of each month of the year 2022\n",
        "hours     = np.arange(24,  dtype=np.uint8)                    # 0 to 23 hours\n",
        "pixeles_x = np.arange(171, dtype=np.uint8)                    # 0 to 171 pixels in x axis\n",
        "pixeles_y = np.arange(171, dtype=np.uint8)                    # 0 to 171 pixels in y axis\n",
        "\n",
        "# Function to generate the dates array with pixels_x and pixels_y for each year\n",
        "def array_per_year(year, year_number):\n",
        "    results = []\n",
        "    for month, days in enumerate(year, 1):\n",
        "        for day in range(1, days + 1):\n",
        "            hx, px, py   = np.meshgrid(hours, pixeles_x, pixeles_y, indexing='ij')\n",
        "            year_actual  = np.full(hx.shape, year_number, dtype=np.uint16)\n",
        "            month_actual = np.full(hx.shape, month, dtype=np.uint8)\n",
        "            day_actual   = np.full(hx.shape, day, dtype=np.uint8)\n",
        "            combination = np.column_stack((hx.ravel(), px.ravel(), py.ravel(), year_actual.ravel(), month_actual.ravel(), day_actual.ravel()))\n",
        "            combination = combination[:,[1,2]]\n",
        "            results.append(combination)\n",
        "    return np.vstack(results)\n",
        "\n",
        "# Output paths for the data files\n",
        "output_paths_dates = {\n",
        "    2017: '/content/sample_data/ETo_climate_2017_dates.npy',\n",
        "    2018: '/content/sample_data/ETo_climate_2018_dates.npy',\n",
        "    2019: '/content/sample_data/ETo_climate_2019_dates.npy',\n",
        "    2020: '/content/sample_data/ETo_climate_2020_dates.npy',\n",
        "    2021: '/content/sample_data/ETo_climate_2021_dates.npy',\n",
        "    2022: '/content/sample_data/ETo_climate_2022_dates.npy'\n",
        "}\n",
        "\n",
        "# List of years and corresponding data\n",
        "years_data = {\n",
        "    2017: year_2017,\n",
        "    2018: year_2018,\n",
        "    2019: year_2019,\n",
        "    2020: year_2020,\n",
        "    2021: year_2021,\n",
        "    2022: year_2022\n",
        "}\n",
        "\n",
        "# Generate arrays and save them if the file does not exist\n",
        "for year, year_days in years_data.items():\n",
        "    outpath = output_paths_dates[year]\n",
        "    if not os.path.exists(outpath):\n",
        "        # Generate the array for the year and save it\n",
        "        results = array_per_year(year_days, year)\n",
        "        np.save(outpath, results.astype(np.uint8), allow_pickle=False)\n",
        "        print(f'Array for {year} saved to {outpath}')\n",
        "    else:\n",
        "        print(f'File for {year} already exists at {outpath}, skipping array generation.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwRWTHRWvpm1"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "import gdown\n",
        "\n",
        "file_id_2017 = '1-lGGwPR-4j-spWeSKB74LD_O7QE0smDa'\n",
        "file_id_2018 = '1-vJ6NeOO22rMu5tbza2wjE5WEqGYWLtR'\n",
        "file_id_2019 = '103zS3AUTCAsyesoeD5YSghRb4i91Xl2C'\n",
        "file_id_2020 = '104V3OvgYhU6s4DAj12lgHMNCknvZWsIA'\n",
        "\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_2017}', 'ETo_weather_2017.npy', quiet=False)\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_2018}', 'ETo_weather_2018.npy', quiet=False)\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_2019}', 'ETo_weather_2019.npy', quiet=False)\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_2020}', 'ETo_weather_2020.npy', quiet=False)\n",
        "\n",
        "# Input paths for the ETo weather labels for train:\n",
        "paths_ETo_weather_labels_train = {\n",
        "    2017: '/content/sample_data/ETo_weather_2017.npy',\n",
        "    2018: '/content/sample_data/ETo_weather_2018.npy',\n",
        "    2019: '/content/sample_data/ETo_weather_2019.npy',\n",
        "    2020: '/content/sample_data/ETo_weather_2020.npy'\n",
        "}\n",
        "\n",
        "\n",
        "# Output paths for the ETo climate Train files:\n",
        "output_paths_ETo_climate_train = {\n",
        "    2017: '/content/sample_data/ETo_climate_2017.npy',\n",
        "    2018: '/content/sample_data/ETo_climate_2018.npy',\n",
        "    2019: '/content/sample_data/ETo_climate_2019.npy',\n",
        "    2020: '/content/sample_data/ETo_climate_2020.npy'\n",
        "}\n",
        "\n",
        "# Generate arrays and save them if the file does not exist\n",
        "for year, path in paths_ETo_weather_labels_train.items():\n",
        "    outpath = output_paths_ETo_climate_train[year]\n",
        "    datepath = output_paths_dates[year]\n",
        "    if not os.path.exists(outpath):\n",
        "        ETo_weather_labels = np.load(path)\n",
        "        print('The ETo weather for train data has a size of: ', ETo_weather_labels.shape)\n",
        "        ETo_weather_labels = ETo_weather_labels[:,-1]\n",
        "        print('The ETo weather labels for train data has a size of: ', ETo_weather_labels.shape)\n",
        "        ETo_climate_dates  = np.load(datepath)\n",
        "        print('The ETo climate dates data has a size of: ', ETo_climate_dates.shape)\n",
        "        ETo_climate = np.concatenate((ETo_climate_dates, ETo_weather_labels.reshape(-1, 1)), axis=1)\n",
        "        np.save(outpath, ETo_climate.astype(np.uint8), allow_pickle=False)\n",
        "        print(f'Array for {year} saved to {outpath}')\n",
        "    else:\n",
        "        print(f'File for {year} already exists at {outpath}, skipping array generation.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVBscl11l_uU"
      },
      "outputs": [],
      "source": [
        "# Load the numpy array of labels and dates joined\n",
        "\n",
        "ETo_climate_2017_train = np.load('/content/sample_data/ETo_climate_2017.npy')\n",
        "ETo_climate_2018_train = np.load('/content/sample_data/ETo_climate_2018.npy')\n",
        "ETo_climate_2019_train = np.load('/content/sample_data/ETo_climate_2019.npy')\n",
        "ETo_climate_2020_train = np.load('/content/sample_data/ETo_climate_2020.npy')\n",
        "\n",
        "ETo_climate_2017_train_pd = pd.DataFrame(ETo_climate_2017_train, columns=['pixel_y', 'pixel_x', 'ETo_weather_labels'])\n",
        "ETo_climate_2018_train_pd = pd.DataFrame(ETo_climate_2018_train, columns=['pixel_y', 'pixel_x', 'ETo_weather_labels'])\n",
        "ETo_climate_2019_train_pd = pd.DataFrame(ETo_climate_2019_train, columns=['pixel_y', 'pixel_x', 'ETo_weather_labels'])\n",
        "ETo_climate_2020_train_pd = pd.DataFrame(ETo_climate_2020_train, columns=['pixel_y', 'pixel_x', 'ETo_weather_labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGyqZvbQRuaw"
      },
      "outputs": [],
      "source": [
        "# Load the numpy arrays of labels and dates for all years\n",
        "years = [2017, 2018, 2019, 2020]\n",
        "ETo_climate_train_paths = {\n",
        "    2017: '/content/sample_data/ETo_climate_2017.npy',\n",
        "    2018: '/content/sample_data/ETo_climate_2018.npy',\n",
        "    2019: '/content/sample_data/ETo_climate_2019.npy',\n",
        "    2020: '/content/sample_data/ETo_climate_2020.npy'\n",
        "}\n",
        "\n",
        "# List to hold pivot results for concatenation\n",
        "ETo_pivot_list = []\n",
        "\n",
        "# Loop over each year, load the data, convert to DataFrame, and pivot\n",
        "for year in years:\n",
        "    # Load the numpy array for the year\n",
        "    ETo_climate_train = np.load(ETo_climate_train_paths[year])\n",
        "\n",
        "    # Convert to pandas DataFrame\n",
        "    ETo_climate_train_pd = pd.DataFrame(ETo_climate_train, columns=['pixel_y', 'pixel_x', 'ETo_weather_labels'])\n",
        "\n",
        "    # Pivot the data to get the count of each label per pixel_x, pixel_y\n",
        "    ETo_pivot = ETo_climate_train_pd.pivot_table(\n",
        "        index=['pixel_x', 'pixel_y'],\n",
        "        columns='ETo_weather_labels',\n",
        "        aggfunc='size',\n",
        "        fill_value=0\n",
        "    ).reset_index()\n",
        "\n",
        "    # Rename the columns to reflect class names\n",
        "    ETo_pivot.columns.name = None  # Remove pivot table's column name\n",
        "    ETo_pivot.columns = ['pixel_x', 'pixel_y'] + [f'class_{int(c)}' for c in ETo_pivot.columns[2:]]\n",
        "\n",
        "    # Add a new column for the year\n",
        "    ETo_pivot['year'] = year\n",
        "\n",
        "    # Append the result to the list\n",
        "    ETo_pivot_list.append(ETo_pivot)\n",
        "\n",
        "    print(f'Pivot table for year {year} created and added to the list.')\n",
        "\n",
        "# Concatenate all pivot tables into a single DataFrame\n",
        "ETo_climate_all_years_train = pd.concat(ETo_pivot_list, ignore_index=True)\n",
        "ETo_climate_all_years_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js4OgvQAfXHe"
      },
      "outputs": [],
      "source": [
        "# Installing sklearn-som package\n",
        "\n",
        "!pip install sklearn-som\n",
        "\n",
        "# Making Custom sklearn-som package\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClusterMixin\n",
        "from sklearn_som.som import SOM as SklearnSOM\n",
        "\n",
        "class CustomSOM(BaseEstimator, ClusterMixin):\n",
        "    def __init__(self, m=1, n=3, dim=4, sigma=1.7, lr=0.1, max_iter=10, random_state=None):\n",
        "        self.m = m\n",
        "        self.n = n\n",
        "        self.dim = dim\n",
        "        self.sigma = sigma\n",
        "        self.lr = lr\n",
        "        self.max_iter = max_iter\n",
        "        self.random_state = random_state\n",
        "        self.model_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.model_ = SklearnSOM(m=self.m, n=self.n, dim=self.dim, lr=self.lr, max_iter=self.max_iter, random_state=self.random_state)\n",
        "        self.model_.fit(X)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model_.predict(X)\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.model_.transform(X)\n",
        "\n",
        "    def score(self, X, y=None):\n",
        "        distancias_punto_centroide = self.transform(X)\n",
        "        distorsion_total = 0\n",
        "        for i in range(len(distancias_punto_centroide)):\n",
        "            distancias_minimas_cuadradas = (np.min(distancias_punto_centroide[i]))**2\n",
        "            distorsion_total += distancias_minimas_cuadradas\n",
        "        return distorsion_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwH5eG5dJBRT"
      },
      "outputs": [],
      "source": [
        "# Set the output path for the normalized file\n",
        "outpath_normalized = '/content/sample_data/ETo_climate_all_years_train_normalized.npy'\n",
        "\n",
        "# Check if the file already exists\n",
        "if not os.path.exists(outpath_normalized):\n",
        "    # If the file does not exist, normalize the data and save it\n",
        "    scaler = StandardScaler()\n",
        "    ETo_climate_normalized = scaler.fit_transform(ETo_climate_all_years_train)\n",
        "\n",
        "    # Save the normalized numpy array\n",
        "    np.save(outpath_normalized, ETo_climate_normalized.astype(np.float32), allow_pickle=False)\n",
        "    print(f'Normalized array saved to {outpath_normalized}')\n",
        "else:\n",
        "    print(f'File already exists at {outpath_normalized}, skipping normalization and saving.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwrNoUA8eY2e"
      },
      "source": [
        "## ETo Climate plot with no optimization of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0EYxglkUkem"
      },
      "outputs": [],
      "source": [
        "# Load the numpy array of labels and dates joined and normalized\n",
        "\n",
        "ETo_climate_normalized = np.load(outpath_normalized)\n",
        "print('ETo climate normalized shape: ', ETo_climate_normalized.shape)\n",
        "\n",
        "# Train the SOM model\n",
        "som_model = CustomSOM(m=4,n=4, dim=39, lr=1, max_iter=1000, random_state=42)\n",
        "som_model.fit(ETo_climate_normalized)\n",
        "\n",
        "ETo_climate_labels  = som_model.predict(ETo_climate_normalized)\n",
        "ETo_climate_data = np.concatenate((ETo_climate_all_years_train, ETo_climate_labels.reshape(-1,1)), axis=1)\n",
        "ETo_climate_pd = ETo_climate_all_years_train\n",
        "ETo_climate_pd['ETo_climate_clusters'] = ETo_climate_labels\n",
        "print(\"The shape of ETo Climate data is: \",ETo_climate_pd.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB9YZYreotNO"
      },
      "source": [
        "## ETo Climate repeatability"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.optimize import linear_sum_assignment  # For the Hungarian algorithm\n",
        "\n",
        "random_state_A =  np.random.randint(1, 1000)  # 137\n",
        "random_state_B =  np.random.randint(1, 1000)  # 946\n",
        "print(\"random_state_A\", random_state_A)\n",
        "print(\"random_state_B\", random_state_B)\n",
        "\n",
        "m_ = 3  # SOM grid size\n",
        "n_ = 3\n",
        "lr_ = 0.5  # Learning rate\n",
        "max_iter_ = 1e7\n",
        "\n",
        "# Training the SOM with random state A\n",
        "som_model_random_state_A = CustomSOM(m=m_, n=n_, dim=39, lr=lr_, max_iter=max_iter_, random_state=random_state_A)\n",
        "som_model_random_state_A.fit(ETo_climate_normalized.astype(np.float128))\n",
        "centroids_random_state_A = som_model_random_state_A.transform(ETo_climate_normalized.astype(np.float128))\n",
        "\n",
        "# Reorder the position of the ETo climate classes with random state A\n",
        "centroids_mean_random_state_A = np.sum(centroids_random_state_A, axis=0)\n",
        "centroids_reorder_random_state_A = centroids_random_state_A[:, np.argsort(centroids_mean_random_state_A)]\n",
        "print(np.sort(centroids_mean_random_state_A))\n",
        "labels_random_state_A = np.argmin(centroids_reorder_random_state_A, axis=1)\n",
        "\n",
        "# Training the SOM with random state B\n",
        "som_model_random_state_B = CustomSOM(m=m_, n=n_, dim=39, lr=lr_, max_iter=max_iter_, random_state=random_state_B)\n",
        "som_model_random_state_B.fit(ETo_climate_normalized.astype(np.float128))\n",
        "centroids_random_state_B = som_model_random_state_B.transform(ETo_climate_normalized.astype(np.float128))\n",
        "\n",
        "# Reorder the position of the ETo climate classes with random state B\n",
        "centroids_mean_random_state_B = np.sum(centroids_random_state_B, axis=0)\n",
        "centroids_reorder_random_state_B = centroids_random_state_B[:, np.argsort(centroids_mean_random_state_B)]\n",
        "print(np.sort(centroids_mean_random_state_B))\n",
        "labels_random_state_B = np.argmin(centroids_reorder_random_state_B, axis=1)\n",
        "\n",
        "# Hungarian algorithm to match clusters\n",
        "# Calculate the cost matrix where each element is the sum of squared differences between centroids\n",
        "cost_matrix = np.zeros((m_ * n_, m_ * n_))\n",
        "for i in range(m_ * n_):\n",
        "    for j in range(m_ * n_):\n",
        "        cost_matrix[i, j] = np.sum((centroids_reorder_random_state_A[:, i] - centroids_reorder_random_state_B[:, j]) ** 2)\n",
        "\n",
        "# Use the Hungarian algorithm to find the optimal assignment\n",
        "row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "# Map labels according to the optimal assignment\n",
        "mapped_labels_B = np.zeros_like(labels_random_state_B)\n",
        "for i, j in zip(row_ind, col_ind):\n",
        "    mapped_labels_B[labels_random_state_B == j] = i\n",
        "\n",
        "# Calculate the difference between mapped labels\n",
        "ETo_climate_diff = labels_random_state_A - mapped_labels_B\n",
        "\n",
        "# Create DataFrames for plotting\n",
        "ETo_climate_pd_random_state_A = ETo_climate_all_years_train.copy()\n",
        "ETo_climate_pd_random_state_A['ETo_climate_clusters'] = labels_random_state_A\n",
        "\n",
        "ETo_climate_pd_random_state_B = ETo_climate_all_years_train.copy()\n",
        "ETo_climate_pd_random_state_B['ETo_climate_clusters'] = mapped_labels_B\n",
        "\n",
        "# Combine the two datasets for easier plotting\n",
        "pivot_A = ETo_climate_pd_random_state_A.pivot_table(index='pixel_y', columns='pixel_x', values='ETo_climate_clusters')\n",
        "pivot_B = ETo_climate_pd_random_state_B.pivot_table(index='pixel_y', columns='pixel_x', values='ETo_climate_clusters')\n",
        "\n",
        "# Calculate percentage of non-zero pixels in the difference matrix\n",
        "total_pixels = len(ETo_climate_diff)\n",
        "non_zero_pixels = np.count_nonzero(ETo_climate_diff)\n",
        "percent_non_zero = (non_zero_pixels / total_pixels) * 100\n",
        "\n",
        "print(f\"Percentage of non-zero pixels: {percent_non_zero:.2f}%\")\n",
        "\n",
        "# Pivot for the difference plot\n",
        "ETo_climate_pd_diff = ETo_climate_all_years_train.copy()\n",
        "ETo_climate_pd_diff['Cluster_Difference'] = ETo_climate_diff\n",
        "pivot_diff = ETo_climate_pd_diff.pivot_table(index='pixel_y', columns='pixel_x', values='Cluster_Difference')\n",
        "\n",
        "# Set up the figure and subplots (1 row, 3 columns)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Plot for Random State A\n",
        "im_A = axes[0].imshow(pivot_A, cmap='viridis', aspect='auto')\n",
        "axes[0].set_title(f'ETo Climate Clusters: Random State {random_state_A}')\n",
        "axes[0].set_xlabel('Long')\n",
        "axes[0].set_ylabel('Lat')\n",
        "fig.colorbar(im_A, ax=axes[0], label=\"Cluster Labels\")\n",
        "\n",
        "# Plot for Random State B\n",
        "im_B = axes[1].imshow(pivot_B, cmap='viridis', aspect='auto')\n",
        "axes[1].set_title(f'ETo Climate Clusters: Random State {random_state_B} (Mapped)')\n",
        "axes[1].set_xlabel('Long')\n",
        "axes[1].set_ylabel('Lat')\n",
        "fig.colorbar(im_B, ax=axes[1], label=\"Cluster Labels\")\n",
        "\n",
        "# Plot for Difference (Random State A - Mapped Random State B)\n",
        "im_diff = axes[2].imshow(pivot_diff, cmap='bwr', aspect='auto', vmin=-np.max(np.abs(ETo_climate_diff)), vmax=np.max(np.abs(ETo_climate_diff)))\n",
        "axes[2].set_title(\"Cluster Differences (A - B)\")\n",
        "axes[2].set_xlabel('Long')\n",
        "axes[2].set_ylabel('Lat')\n",
        "fig.colorbar(im_diff, ax=axes[2], label=\"Difference\")\n",
        "\n",
        "# Adjust layout and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EfD49KIn70_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1uB4Cb3f8SnbRLsgQbol5nCrThxJHyGvG",
      "authorship_tag": "ABX9TyMKNuqDfYgouvIYkVC3lkky",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}